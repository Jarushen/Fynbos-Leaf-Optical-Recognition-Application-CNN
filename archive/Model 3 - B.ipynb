{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import visualkeras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
    "from tensorflow.keras.layers import multiply  # Import the multiply function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_acc = TopKCategoricalAccuracy(k=5, name='top5_acc')\n",
    "top10_acc = TopKCategoricalAccuracy(k=10, name='top10_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 706 images belonging to 23 classes.\n",
      "Found 163 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "root_path = r\"C:\\Users\\Jarushen\\Desktop\\Masters Thesis\\Images\\Fynbos_Dataset_B\"  # Your dataset path\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,  # Adjust for ResNet50\n",
    "    rotation_range=20,  # Increased rotation\n",
    "    width_shift_range=0.2,  # Increased shift range\n",
    "    height_shift_range=0.2,  # Increased shift range\n",
    "    shear_range=0.2,  # Increased shear range\n",
    "    zoom_range=0.2,  # Increased zoom range\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,  # New augmentation\n",
    "    fill_mode='reflect',  # Changed from 'nearest' to 'reflect'\n",
    "    brightness_range=[0.8,1.2],  # New augmentation for brightness\n",
    "    channel_shift_range=20,  # New augmentation for color channels\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "# Load images from directory\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    root_path,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    root_path,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "def squeeze_excite_block(input_tensor, ratio=16):\n",
    "    # Get the number of channels (filters) in the input tensor\n",
    "    channels = input_tensor.shape[-1]\n",
    "    \n",
    "    # Squeeze: Global average pooling\n",
    "    se = GlobalAveragePooling2D()(input_tensor)\n",
    "    se = Reshape((1, 1, channels))(se)\n",
    "    \n",
    "    # Excitation: Two fully connected layers with Batch Normalization\n",
    "    se = Dense(channels // ratio, use_bias=False)(se)\n",
    "    se = BatchNormalization()(se)\n",
    "    se = Activation('relu')(se)\n",
    "    \n",
    "    se = Dense(channels, use_bias=False)(se)\n",
    "    se = BatchNormalization()(se)\n",
    "    se = Activation('sigmoid')(se)\n",
    "    \n",
    "    # Scale the input tensor by the SE block output\n",
    "    x = Multiply()([input_tensor, se])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/20\n",
      "23/23 [==============================] - 2924s 127s/step - loss: 2.8780 - acc: 0.1969 - top_k_categorical_accuracy: 0.4958 - val_loss: 2.6999 - val_acc: 0.1288 - val_top_k_categorical_accuracy: 0.4724\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 2890s 126s/step - loss: 1.6406 - acc: 0.5666 - top_k_categorical_accuracy: 0.8612 - val_loss: 2.3390 - val_acc: 0.3374 - val_top_k_categorical_accuracy: 0.6748\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 3377s 147s/step - loss: 1.2119 - acc: 0.7096 - top_k_categorical_accuracy: 0.9263 - val_loss: 2.0161 - val_acc: 0.4601 - val_top_k_categorical_accuracy: 0.7975\n",
      "Epoch 4/20\n",
      "14/23 [=================>............] - ETA: 20:08 - loss: 0.8838 - acc: 0.8134 - top_k_categorical_accuracy: 0.9737"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Reshape, Multiply, Activation, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Load the base model, pre-trained on ImageNet\n",
    "input_tensor = Input(shape=(299, 299, 3))\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "# Unfreeze the last 30 layers of the base model\n",
    "for layer in base_model.layers[-30:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Add the SE block to the base model's output\n",
    "x = base_model.output\n",
    "x = squeeze_excite_block(x)\n",
    "\n",
    "# Continue with the rest of the network\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "predictions = Dense(len(train_generator.class_indices), activation='softmax')(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with a reduced initial learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001, verbose=1)\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,  # Adjust as needed\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fynbos Leaf Optical Recognition: Optimised Model B Workflow\n",
    "\n",
    "This notebook consolidates all logic for:\n",
    "- Training an optimised ResNet50-based model (Model B) with advanced augmentations and SE block\n",
    "- Evaluating and saving the model\n",
    "- Predicting on new images\n",
    "- Generating SHAP explainability plots for predictions\n",
    "\n",
    "Update paths as needed for your environment (local or Google Drive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SHAP Explainability for Model B ---\n",
    "import shap\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Function to load and preprocess a single image for prediction and SHAP\n",
    "\n",
    "def load_preprocess_image(img_path, target_size=(299, 299)):\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = tf.keras.applications.resnet50.preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "# Example: Predict and explain a new image\n",
    "img_path = 'PATH/TO/NEW/IMAGE.jpg'  # Update this path\n",
    "preprocessed_image = load_preprocess_image(img_path)\n",
    "predictions = model.predict(preprocessed_image)\n",
    "predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "print(f\"Predicted class: {class_names[predicted_class]} ({100*predictions[0][predicted_class]:.2f}%)\")\n",
    "\n",
    "# SHAP DeepExplainer setup (use a small background dataset for efficiency)\n",
    "background = next(train_generator)[0][:10]  # 10 random images from training set\n",
    "explainer = shap.DeepExplainer(model, background)\n",
    "shap_values = explainer.shap_values(preprocessed_image)\n",
    "\n",
    "# Plot SHAP values for the predicted class\n",
    "shap.image_plot(shap_values, preprocessed_image, show=False)\n",
    "plt.title(f\"SHAP Explanation for {class_names[predicted_class]}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
